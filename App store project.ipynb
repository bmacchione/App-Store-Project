{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Apps with the Most Users\n",
    "\n",
    "I am analyzing data from the Apple Store to learn which kinds of apps users are most likely to download.  I am doing this imagining that I work for a company that makes free apps and gets its revenue from ads.  I'm hoping to learn which types of apps get the most users and thus which would make the most sense for this company to work on developing.\n",
    "\n",
    "I'm going to use the Apple Store data set which can be found [here](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps#AppleStore.csv)\n",
    "And the Google Play store data set which can be found [here](https://www.kaggle.com/lava18/google-play-store-apps)\n",
    "\n",
    "The first thing I will do is open both the files and convert them to lists that I can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opened_file = open('AppleStore.csv')\n",
    "from csv import reader\n",
    "read_file = reader(opened_file)\n",
    "apple_data = list(read_file)\n",
    "\n",
    "opened_file2 = open('googleplaystore.csv')\n",
    "from csv import reader\n",
    "read_file2 = reader(opened_file2)\n",
    "google_data = list(read_file2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data\n",
    "I'm going to start with cleaning up the data.\n",
    "From the [discussion](https://www.kaggle.com/lava18/google-play-store-apps/discussion/66015) about the Google data I see that there is a specific row with a missing piece of data which I will want to delete if that's the case.  To double check I'll print it below (the index number is 10473 instead of 10472 as noted in the discussion because I did not delete the header row from my data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life Made WI-Fi Touchscreen Photo Frame',\n",
       " '1.9',\n",
       " '19',\n",
       " '3.0M',\n",
       " '1,000+',\n",
       " 'Free',\n",
       " '0',\n",
       " 'Everyone',\n",
       " '',\n",
       " 'February 11, 2018',\n",
       " '1.0.19',\n",
       " '4.0 and up']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_data[10473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I will delete this row and then print the new row at the same index number to make sure it was deleted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['osmino Wi-Fi: free WiFi',\n",
       " 'TOOLS',\n",
       " '4.2',\n",
       " '134203',\n",
       " '4.1M',\n",
       " '10,000,000+',\n",
       " 'Free',\n",
       " '0',\n",
       " 'Everyone',\n",
       " 'Tools',\n",
       " 'August 7, 2018',\n",
       " '6.06.14',\n",
       " '4.4 and up']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del google_data[10473]\n",
    "google_data[10473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are known duplicate entries in the Google Data set so that is another thing I need to fix while cleaning up the data.  First I will check to see how many duplicates there are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicate Apps 1181\n",
      "\n",
      "\n",
      "Examples of Duplicate Apps ['Quick PDF Scanner + OCR FREE', 'Box', 'Google My Business', 'ZOOM Cloud Meetings', 'join.me - Simple Meetings', 'Box', 'Zenefits', 'Google Ads', 'Google My Business', 'Slack']\n"
     ]
    }
   ],
   "source": [
    "duplicate_apps = []\n",
    "unique_apps = []\n",
    "\n",
    "for app in google_data:\n",
    "    name = app[0]\n",
    "    if name in unique_apps:\n",
    "        duplicate_apps.append(name)\n",
    "    else:\n",
    "        unique_apps.append(name)\n",
    "        \n",
    "print('Number of Duplicate Apps', len(duplicate_apps))\n",
    "print('\\n')\n",
    "print('Examples of Duplicate Apps', duplicate_apps[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 1,181 duplicates in the data set.  I'm going to look at one of the examples to try to get an idea of which entries to keep and which to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Slack', 'BUSINESS', '4.4', '51507', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'August 2, 2018', 'Varies with device', 'Varies with device']\n",
      "['Slack', 'BUSINESS', '4.4', '51507', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'August 2, 2018', 'Varies with device', 'Varies with device']\n",
      "['Slack', 'BUSINESS', '4.4', '51510', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'August 2, 2018', 'Varies with device', 'Varies with device']\n"
     ]
    }
   ],
   "source": [
    "for app in google_data:\n",
    "    name = app[0]\n",
    "    if name == 'Slack':\n",
    "        print(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between the entries appears to be the 4th number (51507, 51507, and 51510).  This number refers to the number of reviews the app received.  In general I would like to use the most recent data to compare the apps so I will keep the entry with the highest number of reviews and delete the other ones.\n",
    "\n",
    "First I'm going to figure out how many unique apps there are so I know what number of apps I'm trying to get in my data set (from the code above I learned that there are 1,181 dupliate apps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected length of clean data set: 9659\n"
     ]
    }
   ],
   "source": [
    "print('Expected length of clean data set:', len(google_data[1:]) - 1181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I expect my clean data set to have 9,659 entries.  I will now create a dictionary where the keys are the names of the apps and the values are the highest number of reviews.  Each app name will be added to the dictionary as the code loops through the data set and the number of reviews will get updated to the highest number.  At the end I will check the length of my dictionary to make sure it is 9,659 like I expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dictionary: 9659\n"
     ]
    }
   ],
   "source": [
    "reviews_max = {}\n",
    "\n",
    "for app in google_data[1:]:\n",
    "    name = app[0]\n",
    "    n_reviews = float(app[3])\n",
    "    if name in reviews_max and reviews_max[name] < n_reviews:\n",
    "        n_reviews = reviews_max[name]\n",
    "    elif name not in reviews_max:\n",
    "        reviews_max[name] = n_reviews\n",
    "        \n",
    "print('Length of Dictionary:', len(reviews_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary length is what I expected and now I have dictionary with each unique app name and the value for the most ratings.  I'm now going to use this dictionary to loop through the original data set and create a cleaned one with all the duplicate entries removed and only the entires with the highest number of reviews remaining.  Some of the duplicate entries have the exact same number of reviews, but I still only want one entry for each app.  Because of this while I'm looping through the data set I will also be creating a list of the names that have already been added to the data set.  That way entries will only be added if they contain the highest number of reviews AND they are not already in the data set.\n",
    "\n",
    "When I'm done I will check the length of my clean data to make sure it is 9,659 like I expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Clean Data Set: 9659\n"
     ]
    }
   ],
   "source": [
    "google_data_clean = []\n",
    "already_added = []\n",
    "\n",
    "for app in google_data[1:]:\n",
    "    name = app[0]\n",
    "    n_reviews = float(app[3])\n",
    "    if n_reviews == reviews_max[name] and name not in already_added:\n",
    "        google_data_clean.append(app)\n",
    "        already_added.append(name)\n",
    "\n",
    "print('Length of Clean Data Set:', len(google_data_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a new clean data set with only unique values.\n",
    "\n",
    "For the analysis I am doing I am only interested in English only apps.  I'm going to write a function that can tell if an app name is English or not.  Because some English app names contains symbols or emojis I'm going to allow apps in the data set with up to 3 non-English characters.  This is not a perfect method but should be good enough for my analysis.  I will test my function on a few app names that I already know are English or not English and see if the function returns the correct values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def is_english(string):\n",
    "    non_english = []\n",
    "    for character in string:\n",
    "        if ord(character) > 127:\n",
    "            non_english.append(character)\n",
    "            if len(non_english) > 3:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(is_english('Instagram'))\n",
    "print(is_english('爱奇艺PPS -《欢乐颂2》电视剧热播'))\n",
    "print(is_english('Instachat 😜'))\n",
    "print(is_english('Docs To Go™ Free Office Suite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My function worked like I expected!  I will now use this function to loop through both the Google and Apple data and create two new clean data sets to work with.  I will then measure the length of each data set to see how many entries are left.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9614\n",
      "7197\n"
     ]
    }
   ],
   "source": [
    "google_data_clean_eng = []\n",
    "apple_data_eng = []\n",
    "\n",
    "for app in google_data_clean:\n",
    "    name = app[0]\n",
    "    if is_english(name):\n",
    "        google_data_clean_eng.append(app)\n",
    "        \n",
    "for app in apple_data[1:]:\n",
    "    name = app[0]\n",
    "    if is_english(name):\n",
    "        apple_data_eng.append(app)\n",
    "\n",
    "print(len(google_data_clean_eng))\n",
    "print(len(apple_data_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
